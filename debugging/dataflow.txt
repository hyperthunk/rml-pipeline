RML Pipeline Data Flow Analysis
Overview of the Data Processing Pipeline
After a detailed analysis of the provided code, I'll outline how data flows through the system to generate RDF triples, focusing on the interdependencies between components and template expansion processes.

Core Data Flow Architecture
Code
Source JSON/XML → Token Stream → Context Building → Template Expansion → Triple Generation → Output
The key components in this flow are:

RML Plan: Generated by the Planner, contains mapping information
String Pool: Stores and interns strings with tiered access patterns
Context Building: Accumulates data values needed for expansion
Template Expansion: Substitutes values from the context into templates
Detailed Component Interaction Analysis
1. RML Plan Structure and Navigation
The plan provides critical structures for identifying relevant data:

F#
// Key pathway for data routing
FastMap.tryFind currentPath pool.Plan.PathToMaps
This lookup is the primary mechanism that routes token data to the appropriate processing components. When a path in the source data matches a path in the plan, the corresponding mapping rules are activated.

2. Context Building Process
In the current implementation, context building happens when tokens are collected:

F#
if not (String.IsNullOrEmpty currentProperty) && objectStack.Count > 0 then
    objectStack.[objectStack.Count - 1].[currentProperty] <- reader.Value
Values are accumulated in the objectStack as nested dictionary structures. When a completion token (usually EndObject) is encountered, this context is sent to the processing tasks:

F#
if token.IsComplete then
    { token with Value = accumulatedData :> obj }
This is a critical point in the pipeline - only at the completion of a JSON object do we have the complete context needed for template expansion.

3. Template Expansion
The template expansion logic is well-structured in Templates.fs:

F#
let expandTemplate (template: string) (context: Context) : TemplateResult =
    // Parse template and substitute values from context
This function identifies placeholders in templates and replaces them with values from the context. The function expects all required values to be present in the context.

4. String Pool Integration Point
A key gap in the current implementation is the lack of explicit string pool integration. The ideal point for string pool interaction would be:

During Planning: Interning all template strings and static values
Before Template Expansion: Interning field names and common paths
After Template Expansion: Optionally interning generated values for frequently reused terms
Data Dependency Analysis
Identified Dependencies
Template to Context Dependencies:

Templates contain references to context values:

Code
"http://example.org/{id}/type/{category}"
These require specific fields (id and category) to be present in the context before expansion.

Join Dependencies:

The JoinTuples in each map reference data from another map:

F#
plan.JoinTuples |> Array.iter (fun joinTuple ->
    processPredicateTuple joinTuple.ParentTuple pool.Output context
    processPredicateTuple joinTuple.ChildTuple pool.Output context
)
This requires data from both parent and child paths to be available in the context.

Critical Gap: Cross-Path Value Collection
The current implementation has a significant gap in how it handles dependencies across different JSON paths. Each path is processed in isolation, with context built only from the data within that path's object structure.

Consider this JSON structure:

JSON
{
  "invoice": {
    "invoiceNumber": "QZY123"
  },
  "insured": {
    "insuredNumber": "INS123"
  }
}
If a template needs both invoiceNumber and insuredNumber, the current model struggles because:

The context for $.invoice only contains invoice data
The context for $.insured only contains insured data
No mechanism exists to merge these contexts for templates that reference both
Dependency Resolution Solutions
1. Cross-Path Context Accumulation
Implement a shared context accumulator that preserves values across paths:

F#
type GlobalContext = {
    PathValues: ConcurrentDictionary<string, Dictionary<string, obj>>
    CompletedPaths: ConcurrentBag<string>
}
Each path's data would be stored with its full path as the key, and template expansion would search across all paths for required values.

2. Dependency-Aware Processing Order
Enhance the planner to generate a processing order that ensures dependent data is processed first:

F#
type ProcessingDependency = {
    SourcePath: string
    RequiredPaths: string[]
    RequiredFields: string[]
}
This would allow the pipeline to:

Identify which paths produce values needed by other paths
Schedule processing to ensure dependencies are resolved before dependents
Hold template expansion until all required data is available
3. Two-Phase Processing
Implement a two-phase processing approach:

Collection Phase: Gather all values from all paths into a global context
Expansion Phase: Perform template expansion once all data is collected
This would guarantee all required values are available before any expansion occurs.

Template Expansion Analysis
The current template expansion system is robust but makes assumptions about context completeness:

F#
match Context.tryFind refName context with
| Some value -> expandParts rest (stringValue :: acc)
| None -> Error (MissingPlaceholder refName)
This fails if any placeholder isn't in the context. For better resilience:

Deferred Expansion: Queue templates with missing values for later processing
Partial Expansion: Allow partial expansion with placeholder indicators for missing values
Default Values: Provide fallback values for missing placeholders
StringPool Integration Design
To properly integrate the StringPool with template expansion:

F#
// During planning - intern all templates
let subjectTemplateId = planningContext.InternString(template, StringAccessPattern.Planning)

// During template expansion
let expandTemplate (template: StringId) (context: Context) (scope: PoolContextScope) : StringId =
    match scope.GetString(template) with
    | Some templateStr ->
        let expandedValue = // perform expansion with context
        scope.InternString(expandedValue, StringAccessPattern.Runtime)
    | None -> StringId.Invalid
This would use the StringPool for all string operations, maintaining the benefits of string interning throughout the pipeline.

Required Enhancements for Robust Dependency Handling
Explicit Dependency Tracking:

F#
type TemplateDependency = {
    TemplateId: StringId
    RequiredFields: string[]
    SourcePath: string
}
Context Completeness Verification:

F#
let isContextComplete (dependencies: TemplateDependency) (context: Context) =
    dependencies.RequiredFields |> Array.forall (fun field -> 
        Context.tryFind field context |> Option.isSome)
Dependency-Based Processing Queue:

F#
type ProcessingItem = {
    Template: StringId
    Context: Context
    Dependencies: TemplateDependency
    Priority: int
}
Scalar Value Handling
You correctly noted that scalar values (atomic values from source data) don't need interning during processing - they're used directly for template expansion. However, if these values are frequently repeated across many triples (like common status codes or categories), interning them after expansion could be beneficial.

The decision point should be:

F#
// When generating a triple from an expanded template
let (subject, predicate, object) = expandedTriple
let subject' = scope.InternString(subject, StringAccessPattern.MediumFrequency)
let predicate' = scope.InternString(predicate, StringAccessPattern.HighFrequency)
let object' = 
    if isCommonValue object then
        scope.InternString(object, StringAccessPattern.MediumFrequency)
    else
        // Don't intern unique literals
        StringId object
Conclusion: Required Architectural Changes
Enhanced Dependency Tracking: The planner needs to explicitly record field dependencies for each template.

Global Context Management: Implement a shared context that accumulates values across all paths.

Dependency-Aware Processing: Sequence processing based on dependencies to ensure required data is available.

StringPool Integration Points: Integrate string interning at planning time and during template expansion.

Two-Phase Processing Model: Separate data collection from template expansion to ensure complete contexts.

These changes would create a robust pipeline that correctly handles interdependent data while maintaining the performance benefits of the string pool and the expressiveness of the template system.